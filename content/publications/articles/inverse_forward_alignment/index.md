---
title: 'Improving LLM Generation with Inverse and Forward Alignment: Reward Modelling, Prompting, Fine-Tuning, and Inference-Time Optimisation'

authors:
  - Hansen Sun
  - Thomas Pouplin
  - admin
  - Tennison Liu
  - Mihaela van der Schaar

date: '2024-12-01T00:00:00Z'
publishDate: '2024-12-01T00:00:00Z'

publication_types: ['1']

publication: NeurIPS 2024 Workshop on System-2 Reasoning at Scale

abstract: |
  We study inverse and forward alignment strategies for large language models across reward modelling, prompting, fine-tuning,
  and inference-time optimisation. The analysis highlights complementary benefits and offers practical guidance for building
  aligned generation systems.

summary: Workshop paper comparing alignment strategies for improving LLM generation quality and controllability.

tags: [Large Language Models, Alignment, Responsible AI]

featured: false

image:
  caption: ''
  focal_point: ''
  preview_only: true
---
This workshop contribution summarises practical recipes for aligning LLMs through inverse and forward optimisation techniques.

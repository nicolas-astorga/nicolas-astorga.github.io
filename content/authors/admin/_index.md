---
# Display name
title: NicolÃ¡s Astorga

# Full name (for SEO)
first_name: NicolÃ¡s
last_name: Astorga

# Status emoji
status:
  icon: ðŸ§ 

# Is this the primary user of the site?
superuser: true

# Role/position/tagline
role: Ph.D. student in Machine Learning

# Organizations/Affiliations to show in About widget
organizations:
  - name: University of Cambridge | van der Schaar Lab
    url: https://www.vanderschaar-lab.com

# Short bio (displayed in user profile at end of posts)
bio: Researching reasoning, Bayesian experimental design, and optimisation in/with LLMs.

# Interests to show in About widget
interests:
  - Large Language Models (LLMs)
  - Reasoning & RL with LLMs
  - Bayesian Experimental Design, Active Learning & Bayesian Optimization
  - Autoformulation for optimization with LLMs
  - Generative Models & Variational Inference

# Education to show in About widget
education:
  courses:
    - course: Ph.D. in Machine Learning
      institution: University of Cambridge (2023â€“present)
    - course: Dual M.Sc. â€” Electrical Engineering; Computer Science
      institution: University of Chile (2020â€“2023)
    - course: B.Sc. â€” Computer, Electrical & Mechanical Engineering (Three Major)
      institution: University of Chile (2013â€“2019)

# Social/Academic Networking
social:
  - icon: envelope
    icon_pack: fas
    link: '/#contact'
  - icon: github
    icon_pack: fab
    link: https://github.com/nicolas-astorga
  - icon: google-scholar
    icon_pack: ai
    link: https://scholar.google.com/citations?user=oLiBK8cAAAAJ

# Enter email to display Gravatar (if Gravatar enabled in Config)
# email:

# Highlight the author in author lists? (true/false)
highlight_name: true
---

Iâ€™m a Ph.D. student at the University of Cambridge advised by Prof. Mihaela van der Schaar. My research explores **reasoning**, **Bayesian experimental design**, and **optimization** in the context of **LLMs**. Previously, I completed a two M.Sc. in Electrical Engineering and Computer Science at the University of Chile. I also worked as an ML Engineer at ALeRCE, and interned at Harvard IACS.

My core ML interest is LLM research: leveraging modelsâ€™ inductive biases to drive exploration and exploitation, then using the resulting experience to improve those biases (via in-context learning or training). Because explorationâ€“exploitation couples information gathering with a goal, search methods and efficient experimentation also result interesting. These ideas admit many implementations, which is part of the fun. I love that LLMsâ€”and deep learning more broadlyâ€”are built to search and learn.